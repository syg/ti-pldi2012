
\section{Implementation}
\label{sec:implementation}

We have implemented this analysis for SpiderMonkey, the JavaScript engine
in Firefox.
We have also modified the engine's JIT compiler, JaegerMonkey, to use
inferred type information when generating code.
Without type information, JaegerMonkey generates code in a fairly mechanical
translation from the original SpiderMonkey bytecode for a script.
Using type information, we were able to improve on this in several ways:

\begin{itemize}

\item Values with statically known types can be tracked in JIT-compiled code
using an untyped representation.
Encoding the type in a value requires significant memory traffic
if the type is stored separately from the actual data in a value
(as it is in SpiderMonkey),
or marshaling overhead if a single word is used to encode values.

An untyped representation stores just the data component of a value ---
a raw pointer for objects and strings, and raw bits for other value types ---
without explicitly encoding the type.
Additionally, knowing the type of a value statically eliminates many
dynamic type tests which the code would otherwise require.

\item Several classical compiler optimizations were added,
including linear scan register allocation, loop invariant code motion
and function call inlining.

These optimizations could be applied without
having static type information.
Doing so is, however, far more difficult and far less effective than in
the case where types are known.\footnote{For example, loop invariant code motion depends
on knowing whether operations
are idempotent, while in general JavaScript operations are not,
and register allocation requires types to determine whether values should
be stored in general purpose or floating point registers.}

\end{itemize}

In \Section\ref{sec:recompilation} we describe how we handle dynamic recompilation in
response to type changes, and in \Section\ref{sec:memory} we describe the techniques
used to manage analysis memory usage.

\subsection{Recompilation}
\label{sec:recompilation}

As described in \Section\ref{sec:example}, computed type information can change as
a result of runtime checks, newly analyzed code or other dynamic behavior.
For compiled code to rely on this type information, we must be able
to recompile the code in response to changes in types while that code is
still running.

Compiling a function will only query type information, and will not change it.
The type sets queried can be for any expression %or lvalue
in the program,
independent of the function being compiled,
and the dependencies of a compiled function need to be encoded efficiently
and precisely, to avoid unnecessary recompilation.

As each function is compiled, we keep track of all type information queried
by the compiler.
Afterwards, the dependencies are encoded as recompilation constraints,
in the same manner as the type constraints generated during analysis itself.
When new types are possible for the type sets associated with a recompilation
constraint, the compiled function is marked for recompilation.
We represent the contents of type sets explicitly and eagerly resolve
constraints, so that new types immediately
trigger recompilation with little overhead.

%When starting this project, the best strategy for recompilation
%was not apparent.
%We originally tried to immediately generate new JIT code for a script,
%scan the stack for calls made from the script's old code and patch the
%associated return addresses with the corresponding addresses in the
%new code.
%This is a simple design and easy to implement, but had a few major problems:

%\begin{itemize}

%\item When recompiling a script, the compiler had to be sure to generate
%a superset of the calls which appeared in the original script.
%This places complicated constraints on the optimizations that
%can be done when compiling or recompiling.

%\item If many type sets change within a short time period (but not all at
%the same time), the recompiler could thrash and repeatedly recompile a
%method, incurring large overhead.

%\end{itemize}

%To address these, we changed the design so that return addresses of
%calls made by the old code were replaced with the addresses of
%a few specialized trampolines.
%The JIT code for the script is discarded, and the script is not
%immediately recompiled.
%After control returns to the trampolines, they use metadata added to
%the calling frame to assemble
%a valid state for the JavaScript engine's bytecode interpreter,
%and resume execution in the interpreter.

Immediate recompilation causes problems such as frequent recompilation
(thrashing) and complex constraints on optimizations. The details of these
problems are elided for brevity. When a function is marked for recompilation, we
do not immediately recompile but instead discard the JIT code for the
function. Functions which have had their JIT code discarded resume execution
in the interpreter.

We do not compile functions until after a certain number of calls or loop back
edges are taken, and these counters are reset whenever discarding JIT code
for the function.
Once the function warms back up, it will be recompiled using the new type information
in the same manner as its initial compilation.

Since the initial compilation and recompilations occur independently
from one another, the compiler is free to implement any optimization it
wants provided that a valid interpreter state can always be recovered ---
only information which is provably dead (regardless of currently known types)
can be discarded.
In practice this restriction has not been a problem for
implementing optimizations.

Additionally, since the function will execute in the interpreter some before
being recompiled, thrashing is reduced, and thresholds can be adjusted to
increase the amount of time spent interpreting if thrashing is detected
dynamically.

\subsection{Memory Management}
\label{sec:memory}

Two major goals of JIT compilation in a web browser stand in stark contrast
to one another: generate code that is as fast as possible, and use as little
memory as possible.
JIT code can consume a large amount of memory, and the type sets and constraints
computed by our analysis consume even more.
We reconcile this conflict by observing how browsers are used in practice:
to surf the web.
The web page being viewed, content being generated, and JavaScript code being
run are constantly changing.
The compiler and analysis need to not only quickly adapt to new scripts that are
running, but also to quickly discard regenerable data associated with
old scripts that are no longer running much, even if the old scripts are still
reachable and not subject to garbage collection.

We do this with a simple trick:
on every garbage collection, we throw away all JIT code and as much analysis
information as possible.
All inferred types are functionally determined from a small core of type
information:
type sets for the properties of objects, function arguments, the observed
type sets associated with barrier constraints and the semantic triggers which
have been tripped.
All type constraints and all other type sets are discarded, notably the type sets
describing the intermediate expressions in a function without barriers on them.
This constitutes the great majority of the memory allocated for analysis.
Should the involved functions warm back up and require recompilation,
they will be reanalyzed. In combination with the retained type information,
the complete analysis state for the function is then recovered.

In Firefox, garbage collections typically happen every several seconds.
If the user is quickly changing pages or tabs, unused JIT code and analysis
information will be quickly destroyed.
If the user is staying on one page, active scripts may be repeatedly
recompiled and reanalyzed, but the timeframe between collections keeps this
as a small portion of overall runtime.
When many tabs are open (the case where memory usage is most important
for the browser), analysis information typically accounts
for less than 2\% of the browser's overall memory usage.

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "main"
%%% End: 
