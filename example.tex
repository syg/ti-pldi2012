
\section{Example and Motivation}
\label{sec:example}

\begin{figure}
\begin{lstlisting}[xleftmargin=18pt]
function Box(v) {
  this.f = v;
}

function use(a) {
  var res = 0;
  for (var i = 0; i < 1000; i++) {
    var v = a[i].f;
    res = res + v;
  }
  return res;
}

function main() {
  var a = [];
  for (var i = 0; i < 1000; i++)
    a[i] = new Box(10);
  use(a);
}
\end{lstlisting}
\caption{Motivating Example}
\label{fig:motivating-example}
\end{figure}

Consider the example JavaScript program in Figure~\ref{fig:motivating-example}.
This program constructs an array of \code{Box} objects wrapping integer
values, and calls a \code{use} method which adds up the contents of all
those \code{Box} objects.
No types are specified for any of the variables or other values used
in this program, in keeping with JavaScript's dynamically typed nature.
However, most operations in this program interact with type information,
and knowledge of the involved types is needed for the code to
be efficiently compiled.

In particular, we are interested in the addition
\code{res + v} on line 9.
In JavaScript, it is legal to add any two values together, with the
operands coerced if necessary into strings or numbers and a string
concatenation or numeric addition performed.

Without static information about the types of \code{res} and \code{v},
a JIT compiler must emit code to check the types of these two values,
with several paths handling the possible combinations of
operand types.
Moreover, every time values are copied around, the compiler must emit
code to keep track of the types of the involved values, using either
a separate type tag for the value or a specialized marshaling format.
This incurs a large runtime overhead on the generated code,
greatly increases the complexity of the compiler,
and makes effective implementation of important optimizations like
register allocation and loop invariant code motion much harder.

If we know the types of \code{res} and \code{v}, we can compile
code which performs an integer addition with no type checks or need
to keep track of the types of \code{res} and \code{v}.
With static knowledge of all types involved in the program, the compiler can
in many cases generate code similar to that produced for a statically
typed language such as Java, and using similar optimizations.

We can infer possible types for \code{res} and \code{v} statically,
by reasoning about the effect the program's assignments
and operations have on values produced later.
This is illustrated below (for brevity, this reasoning does not consider
the possibility of functions \code{Box} and \code{use} being overwritten).

\begin{enumerate}
\item On line 17, \code{main} passes an integer when constructing \code{Box}
      objects. On line 2, \code{Box} assigns its parameter to the result's
      \code{f} property. Thus, \code{Box} objects can have an
      integer property \code{f}.
\item Also on line 17, \code{main} assigns a \code{Box} object to an element
      of \code{a}. On line 15, \code{a} is assigned an array literal,
      so the elements of that literal could be \code{Box} objects.
\item On line 18, \code{main} passes \code{a} to \code{use}, so \code{a}
      within \code{use} can refer to the array created line 15.
      When \code{use} accesses an element of \code{a} on line 8,
      per \#2 the result can be a \code{Box} object.
\item On line 8, property \code{f} of a value at \code{a[i]} is assigned to \code{v}.
      Per \#3 \code{a[i]} can be a \code{Box} object, and per \#1 the
      \code{f} property can be an integer. Thus, \code{v} can be an integer.
\item On line 6, \code{res} is assigned an integer. Since \code{v} can be
      an integer, \code{res + v} can be an integer.
      When that addition is assigned to \code{res} on line 9, the assigned
      type is consistent with the known possible types of \code{res}.
\end{enumerate}

This reasoning can be captured with inclusion constraints; we compute
sets of possible types for each value and model the flow of values between
these sets as subset relationships.
To be useful for compilation, we need to know not just {\bf some}
possible types for variables, but {\bf all} possible types.
In this sense, the static inference above is unsound, and does not account
for all possible behaviors of the program.
A few such behaviors are described below.

\begin{itemize}

\item The read of \code{a[i]} may access a {\it hole} in the array.
Out of bounds array accesses in JavaScript do not throw an exception,
but instead produce the \code{undefined} value if the array's prototype
does not have a matching property.
Such holes can also be in the middle of an array;
assigning to just \code{a[0]} and \code{a[2]} leaves a missing
value at \code{a[1]}.

\item Similarly, the read of \code{a[i].v} may be accessing a property
which is not actually held by \code{a[i]} or a prototype, and may produce the
\code{undefined} value.

\item The addition \code{res + v} may overflow.
JavaScript has a single number type which does not distinguish between
integers and doubles.
However, it is extremely important for performance that JavaScript compilers
distinguish integer and double values, and try to represent numbers as
integers wherever possible.
An addition of two integers may overflow and produce a number which can
only be represented as a double.

\end{itemize}

In some cases these behaviors can be proved statically to not occur,
but usually they cannot be ruled out.
If we try to capture these behaviors statically, many element or property
accesses will be marked as possibly undefined and many integer operations
will be marked as possibly overflowing.
The resulting type information will be too imprecise to be useful for
JIT compilation.

We address this problem by combining static inference about types
with targeted dynamic type updates.
Behaviors which are not accounted for statically must be caught dynamically,
which will trigger modification of the inferred types to reflect
those new behaviors.
If \code{a[i]} accesses a hole, the inferred types for the result must be
marked as possibly undefined.
If \code{res + v} overflows, the inferred types for the result must be
marked as possibly a double.

With or without analysis, the generated code needs to test for array holes
and integer overflow in order to correctly model the semantics of the language.
These and other dynamic type updates are {\it semantic triggers}:
they are placed on already existing, rarely taken execution paths,
and incur a cost to update the inferred types only the first time that
execution path is taken for an operation.

The presence of these triggers illustrates the key invariant our analysis
preserves: inferred types must conservatively model all types for
variables and object properties which currently exist, but not those
which could exist in the future.
This has important implications:

\begin{itemize}

\item The program can be analyzed incrementally, as code starts to execute.
Code which has never executed does not need to be analyzed.
This is necessary for JavaScript due to dynamic code loading
and generation through
\code{eval()} and similar mechanisms, and is also important for reducing
analysis time on websites, which often load several megabytes of
code and only execute a fraction of it.

\item Assumptions about types made by the JIT compiler can be invalidated
at almost any time.
This affects the correctness of the JIT'ed code, and the virtual machine
must be able to support recompiling or discarding JIT'ed code at any time,
including (and especially) when that code is on the stack.
We discuss our handling of this in Section~\ref{XXX}.

\end{itemize}

This invariant is also critical to our handling of polymorphic code
within a program.
Consider the following extension to the example program:

\begin{lstlisting}[numbers=none]
function other() {
  var v = new Box("hello!");
}
\end{lstlisting}

Executing the \code{other()} function will cause \code{Box} objects
to be created which hold strings,
illustrating the use of \code{Box} as a polymorphic structure.
Our analysis does not distinguish \code{Box} objects created in different
places, and the result of the \code{a[i].v} access in \code{use()} will
be regarded as potentially producing a string.
Naively, solving the constraints produced by the analysis will mark
\code{a[i].v}, \code{v}, \code{res + v}, and \code{res} as all producing
either an integer or a string, even if \code{use()} behaves in a monomorphic
fashion and only ever works on \code{Box} objects containing integers.

This problem of imprecision leaking across the program is serious, as even
if a program is by and large monomorphic the precision of analysis results on
it can easily be poisoned by a small amount of polymorphic code.

We deal with uses of polymorphic structures and functions using runtime checks.
At all element and property accesses, we keep track of both the set of
types which {\bf could} be observed for the access and the set of types
which {\bf has} been observed.
The former will be a superset of the latter, and if the two are different then
we insert a runtime check, a {\it type barrier} to check for conformance
between whatever value is read out and the observed type set.
Mismatches lead to updates of the observed type set, and because our
invariant only requires inferred types to reflect observed types, it is
safe to only solve analysis constraints with respect to the observed types.

For the example program, a type barrier is required on the \code{a[i].f} access
on line 8, and nowhere else. The barrier will test that the value being read
is an integer, and if a string shows up sometime due to a call to \code{use}
outside of \code{main} then the possible types of the \code{a[i].f} access
will be updated and \code{res} and \code{v} will be marked as possibly
strings by resolving the analysis constraints.

Type barriers differ from the semantic triggers described earlier in that
they are not required by the semantics of the language and do not need to
be performed if our analysis is not being used.
These checks are cheap, though, and we are effectively betting that the
cost incurred by performing them will be more than made up for by
the better code generated using more precise type information.
We have found this to be the case (Section~\ref{XXX}).

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "main"
%%% End: 

