
\section{The Need for Hybrid Analysis}
\label{sec:example}

\begin{figure}
\begin{lstlisting}[xleftmargin=18pt]
function Box(v) {
  this.p = v;
}

function use(a) {
  var res = 0;
  for (var i = 0; i < 1000; i++) {
    var v = a[i].p;
    res = res + v;
  }
  return res;
}

function main() {
  var a = [];
  for (var i = 0; i < 1000; i++)
    a[i] = new Box(10);
  use(a);
}
\end{lstlisting}
\caption{Motivating Example}
\label{fig:motivating-example}
\end{figure}

Consider the example JavaScript program in Figure~\ref{fig:motivating-example}.
This program constructs an array of \code{Box} objects wrapping integer
values, then calls a \code{use} function which adds up the contents of all
those \code{Box} objects.
No types are specified for any of the variables or other values used
in this program, in keeping with JavaScript's dynamically-typed nature.
Nevertheless, most operations in this program interact with type information,
and knowledge of the involved types is needed to compile efficient code.

In particular, we are interested in the addition \code{res + v} on line 9.  In
JavaScript, addition coerces the operands into strings or numbers if
necessary. String concatenation is performed for the former, and numeric
addition for the latter.

Without static information about the types of \code{res} and \code{v},
a JIT compiler must emit code to handle all possible combinations of
operand types.
Moreover, every time values are copied around, the compiler must emit
code to keep track of the types of the involved values, using either
a separate type tag for the value or a specialized marshaling format.
This incurs a large runtime overhead on the generated code,
greatly increases the complexity of the compiler,
and makes effective implementation of important optimizations like
register allocation and loop invariant code motion much harder.

If we knew the types of \code{res} and \code{v}, we can compile
code which performs an integer addition without the need to check or
to track the types of \code{res} and \code{v}.
With static knowledge of all types involved in the program, the compiler can
in many cases generate code similar to that produced for a statically-typed
 language such as Java, with similar optimizations.

We can infer possible types for \code{res} and \code{v} statically
by reasoning about the effect the program's assignments
and operations have on values produced later.
This is illustrated below (for brevity, we do not consider
the possibility of \code{Box} and \code{use} being overwritten).

\begin{enumerate}
\item On line 17, \code{main} passes an integer when constructing \code{Box}
      objects. On line 2, \code{Box} assigns its parameter to the result's
      \code{p} property. Thus, \code{Box} objects can have an
      integer property \code{p}.
\item Also on line 17, \code{main} assigns a \code{Box} object to an element
      of \code{a}. On line 15, \code{a} is assigned an array literal,
      so the elements of that literal could be \code{Box} objects.
\item On line 18, \code{main} passes \code{a} to \code{use}, so \code{a}
      within \code{use} can refer to the array created line 15.
      When \code{use} accesses an element of \code{a} on line 8,
      per \#2 the result can be a \code{Box} object.
\item On line 8, property \code{p} of a value at \code{a[i]} is assigned to \code{v}.
      Per \#3 \code{a[i]} can be a \code{Box} object, and per \#1 the
      \code{p} property can be an integer. Thus, \code{v} can be an integer.
\item On line 6, \code{res} is assigned an integer. Since \code{v} can be
      an integer, \code{res + v} can be an integer.
      When that addition is assigned to \code{res} on line 9, the assigned
      type is consistent with the known possible types of \code{res}.
\end{enumerate}

This reasoning can be captured with inclusion constraints; we compute
sets of possible types for each expression and model the flow between
these sets as subset relationships.
To compile correct code, we need to know not just \emph{some}
possible types for variables, but \emph{all} possible types.
In this sense, the static inference above is incomplete: it does not account
for all possible behaviors of the program.
A few such behaviors are described below.

\begin{itemize}

\item The read of \code{a[i]} may access a {\it hole} in the array.
Out of bounds array accesses in JavaScript produce the
\code{undefined} value if the array's prototype
does not have a matching property.
Such holes can also be in the middle of an array;
assigning to just \code{a[0]} and \code{a[2]} leaves a missing
value at \code{a[1]}.

\item Similarly, the read of \code{a[i].v} may be accessing a missing property
and may produce the \code{undefined} value.

\item The addition \code{res + v} may overflow.
JavaScript has a single number type which does not distinguish between
integers and doubles.
However, it is extremely important for performance that JavaScript compilers
distinguish the two and try to represent numbers as
integers wherever possible.
An addition of two integers may overflow and produce a number which can
only be represented as a double.

\end{itemize}

In some cases these behaviors can be proven not to occur,
but usually they cannot be ruled out.
A standard solution is to capture these behaviors statically, but this is
unfruitful. The static analysis must be sound, and to be sound in
light of highly dynamic behaviors is to be conservative: many element or property
accesses will be marked as possibly undefined, and many integer operations
will be marked as possibly overflowing.
The resulting type information would be too imprecise to be useful for
optimization.

Our solution, and our key technical novelty, is to combine
unsound static inference
of the types of expressions and heap values
with targeted dynamic type updates.
Behaviors which are not accounted for statically must be caught dynamically,
modifying inferred types to reflect
those new behaviors if caught.
If \code{a[i]} accesses a hole, the inferred types for the result must be
marked as possibly \code{undefined}.
If \code{res + v} overflows, the inferred types for the result must be
marked as possibly a double.

With or without analysis, the generated code needs to test for array holes
and integer overflow in order to correctly model the semantics of the language.
We call dynamic type updates based on these events {\it semantic triggers}:
they are placed on rarely taken execution paths
and incur a cost to update the inferred types only the first time that
path is taken.

The presence of these triggers illustrates the key invariant our analysis
preserves:
\begin{quote}
Inferred types must conservatively model all types for
variables and object properties which currently exist and have existed
in the past, but not those
which could exist in the future.
\end{quote}
This has important implications:

\begin{itemize}

\item The program can be analyzed incrementally, as code starts to execute.
Code which does not execute need not be analyzed.
This is necessary for JavaScript due to dynamic code loading
and generation. It is also important for reducing
analysis time on websites, which often load several megabytes of
code and only execute a fraction of it.

\item Assumptions about types made by the JIT compiler can be invalidated
at almost any time.
This affects the correctness of the JIT-compiled code, and the virtual machine
must be able to recompile or discard code at any time,
especially when that code is on the stack.

\end{itemize}

Dynamic checks and the key invariant are also critical to our handling of polymorphic code
within a program. Suppose somewhere else in the program we have
\lstinline{new Box("hello!")}.
Doing so will cause \code{Box} objects
to be created which hold strings,
illustrating the use of \code{Box} as a polymorphic structure.
Our analysis does not distinguish \code{Box} objects created in different
places, and the result of the \code{a[i].v} access in \code{use} will
be regarded as potentially producing a string.
Naively, solving the constraints produced by the analysis will mark
\code{a[i].v}, \code{v}, \code{res + v}, and \code{res} as all producing
either an integer or a string, even if \code{use}'s runtime behavior is actually monomorphic
and only works on \code{Box} objects containing integers.

This problem of imprecision leaking across the program is serious: even
if a program is mostly monomorphic, analysis precision can
easily be poisoned by a small amount of polymorphic code.

We deal with uses of polymorphic structures and functions using runtime checks.
At all element and property accesses, we keep track of both the set of
types which \emph{could} be observed for the access and the set of types
which \emph{has been} observed.
The former will be a superset of the latter, and if the two are different then
we insert a runtime check, a {\it type barrier}, to check for conformance
between the resultant value and the observed type set.
Mismatches lead to updates of the observed type set.
% and because our
%invariant only requires inferred types to reflect observed types, it is
%safe to only solve analysis constraints with respect to the observed types.

For the example program, a type barrier is required on the \code{a[i].p} access
on line 8, and nowhere else. The barrier will test that the value being read
is an integer. If a string shows up due to a call to \code{use}
outside of \code{main}, then the possible types of the \code{a[i].p} access
will be updated, and \code{res} and \code{v} will be marked as possibly
strings by resolving the analysis constraints.

Type barriers differ from the semantic triggers described earlier in that
the tests they perform are not required by the language
and do not need to
be performed if our analysis is not being used.
We are effectively betting that the required barriers
pay for themselves by enabling generation of better code using more precise type information.
We have found this to be the case in practice (\Section\ref{sec:barrier_cost}, \Section\ref{sec:without_barriers}).

\subsection{Comparison with other techniques}

The reader may question, ``Why not use more sophisticated static analyses that
produce more precise results?''
Our choice for the static analysis to not distinguish \code{Box} objects
created in different places is deliberate.
To be useful in a JIT setting, the analysis must be fast, and the time
and space used by the analysis quickly degrade as complexity increases.
Moreover, there is a tremendous variety of polymorphic behavior seen in
JavaScript code in the wild, and to retain precision even the
most sophisticated static analysis would need to fall back to dynamic
checks some of the time.

Interestingly, \emph{less} sophisticated static
analyses do not fare well either. Unification-based analyses undermine the
utility of dynamic checks; precision is unrecoverable despite dynamic
monitoring.

More dynamic compilation strategies
generate type specialized code based on profiling
information, without static knowledge of possible argument or heap types
\cite{ChambersThesis, Chambers89}.
Such techniques will determine the types of expressions with similar
precision to our analysis,
but will always require type checks on function arguments or when reading
heap values.
With knowledge of all possible types, we only need type checks at accesses
with type barriers, a difference which significantly improves performance
(\Section\ref{sec:barrier_cost}).

We believe that our partitioning of static and dynamic analysis
is a sweet spot for JIT compilation of a highly dynamic language.
Our main technical contribution is a hybrid inference algorithm for the
entirety of JavaScript, using inclusion constraints to unsoundly infer
types extended with runtime semantic triggers to generate sound type
information, as well as type barriers to efficiently and precisely handle
polymorphic code. Our practical contributions include both an
implementation of our algorithm and a realistic evaluation. The implementation
is integrated with the JIT compiler used in Firefox and is of production
quality. Our evaluation has various metrics showing the effectiveness of the
analysis and modified compiler on benchmarks as well as popular websites,
games and demos.

%\begin{itemize}

%\item We describe the use of inclusion constraints to incompletely infer
%possible types in JavaScript programs, and the runtime extension with semantic
%triggers to generate sound type information.

%\item A hybrid inference algorithm for the entirety of JavaScript, using inclusion
%constraints to incompletely infer types extended with runtime semantic triggers
%to generate sound type information.

%\item The use of type barriers to efficiently and precisely handle
%polymorphic code.

%\item A production implementation integrated with the JIT compiler used in
%Firefox.
%, and describe the issues encountered and fixed while preparing
%the analysis for release.

%\item A realistic evaluation with various metrics showing the effectiveness of the analysis
%and modified compiler on benchmarks as well as popular websites.

%\end{itemize}

The remainder of the paper is organized as follows. In \Section\ref{sec:analysis}
we describe the static and dynamic aspects of our
analysis. In \Section\ref{sec:implementation} we outline implementation of the
analysis as well as integration with the JavaScript JIT compiler inside
Firefox. In \Section\ref{sec:evaluation} we present empirical
results. In \Section\ref{sec:related-work} we discuss related work, and in
\Section\ref{sec:conclusion} we conclude.

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "main"
%%% End: 

